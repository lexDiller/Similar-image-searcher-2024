# Система поиска изображений, похожих по смыслу

Этот репозиторий содержит **Jupyter ноутбуки** с извлечением и сравнением признаков изображений. Основная цель — реализовать различные подходы(в последствии применить каскад этих подходов) для поиска похожих изображений и классификации с помощью современных моделей глубокого обучения (ConvNeXt, BLIP, Qwen2-VL и др.).

## Ноутбуки и подходы

### 1. `only_convnext.ipynb`
- **Задача**: извлечение визуальных признаков из изображений с помощью ConvNeXt.
- **Методы**:
  - Построение эмбеддингов и использование библиотеки Annoy для поиска ближайших соседей.
  - Рассчитывается метрика **MAP@10** для оценки качества рекомендаций.
- **Особенности**:
  - Класс `ImageEncoder` для получения эмбеддингов.
  - Стратегия голосования по ближайшим соседям для определения категории.
  - Итоговая точность: ~0.74–0.75 (MAP@10).

### 2. `combined_emb.ipynb`
- **Идея**: объединить визуальные и текстовые эмбеддинги для более точного поиска похожих изображений.
- **Компоненты**:
  - ConvNeXt для визуальной части.
  - BLIP для генерации описаний (caption) к изображениям.
  - SentenceTransformer для получения текстовых эмбеддингов.
- **Принцип**:
  - Конкатенация вектора от ConvNeXt с текстовым вектором (BLIP → SentenceTransformer).
  - Улучшенная точность поиска за счёт учёта семантического описания.

### 3. `make_submisson.ipynb`
- **Цель**: сформировать финальный сабмит на основе обученных моделей.
- **Содержание**:
  - Пайплайн обработки тестового датасета.
  - Объединение результатов разных стратегий.
  - Форматирование выходных данных (CSV, JSON и т.п.) для отправки на платформу/конкурс.

### 4. `vlm.ipynb`
- **Тема**: Мультимодальной моделью Qwen2-VL.
- **Подход**:
  - Извлечение эмбеддингов, учитывающих как визуальную, так и текстовую информацию.
  - Анализ внутреннего представления модели (attention, hidden states).
- **Преимущества**:
  - Возможность zero-shot классификации и более глубокого семантического анализа.
